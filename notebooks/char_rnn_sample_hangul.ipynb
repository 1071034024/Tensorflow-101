{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages Imported\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Import Packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import string\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "from TextLoader import *\n",
    "from Hangulpy import *\n",
    "print (\"Packages Imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading preprocessed files\n",
      "type of 'data_loader' is <type 'dict'>, length is 78\n",
      "\n",
      "\n",
      "data_loader.vocab looks like \n",
      "{'\\x81': 15, '\\x80': 53, '\\x83': 49, '\\x82': 20, '\\x85': 1, '\\x84': 5, '\\x87': 7, '\\x86': 25, '\\x89': 43, '\\x88': 18, '\\x8b': 45, '\\n': 22, '\\x8d': 34, '\\x8c': 33, '\\x8f': 8, '\\x8e': 17, '\\x91': 37, '\\x90': 24, '\\x93': 14, '\\x92': 74, '\\x95': 21, '\\x94': 23, '\\x97': 13, '\\x96': 42, '\\x99': 55, '\\x98': 29, '\\x9b': 41, '\\x9a': 36, '\\x9d': 39, '\\x9c': 19, '\\x9f': 38, '\\x9e': 63, '\\xa1': 12, ' ': 6, '\\xa3': 10, '\\xa2': 28, '\\xa5': 3, \"'\": 51, '>': 77, ')': 48, '(': 47, '-': 67, ',': 30, '.': 27, '\\xb1': 11, '0': 75, '\\xb3': 64, '\\xb2': 32, '\\xb5': 52, '\\xb4': 2, '\\xb7': 16, '\\xb6': 46, '\\xb9': 9, '\\xb8': 35, '\\xbb': 57, '\\xba': 50, '\\xbc': 56, '?': 44, '4': 69, '!': 54, '\\x8a': 26, '7': 71, '\\xa0': 40, '6': 61, '3': 68, '9': 66, '\"': 31, '8': 70, '2': 62, '_': 73, '\\xe1': 4, '\\xe3': 0, ':': 59, '\\x1a': 76, '1': 60, '\\xbf': 58, '\\xbe': 72, '5': 65} \n",
      "\n",
      "\n",
      "type of 'data_loader.chars' is <type 'tuple'>, length is 78\n",
      "\n",
      "\n",
      "data_loader.chars looks like \n",
      "('\\xe3', '\\x85', '\\xb4', '\\xa5', '\\xe1', '\\x84', ' ', '\\x87', '\\x8f', '\\xb9', '\\xa3', '\\xb1', '\\xa1', '\\x97', '\\x93', '\\x81', '\\xb7', '\\x8e', '\\x88', '\\x9c', '\\x82', '\\x95', '\\n', '\\x94', '\\x90', '\\x86', '\\x8a', '.', '\\xa2', '\\x98', ',', '\"', '\\xb2', '\\x8c', '\\x8d', '\\xb8', '\\x9a', '\\x91', '\\x9f', '\\x9d', '\\xa0', '\\x9b', '\\x96', '\\x89', '?', '\\x8b', '\\xb6', '(', ')', '\\x83', '\\xba', \"'\", '\\xb5', '\\x80', '!', '\\x99', '\\xbc', '\\xbb', '\\xbf', ':', '1', '6', '2', '\\x9e', '\\xb3', '5', '9', '-', '3', '4', '8', '7', '\\xbe', '_', '\\x92', '0', '\\x1a', '>') \n"
     ]
    }
   ],
   "source": [
    "data_dir    = \"data/nine_dreams\"\n",
    "batch_size  = 50\n",
    "seq_length  = 50\n",
    "data_loader = TextLoader(data_dir, batch_size, seq_length)\n",
    "# This makes \"vocab.pkl\" and \"data.npy\" in \"data/nine_dreams\"   \n",
    "#  from \"data/nine_dreams/input.txt\" \n",
    "vocab_size = data_loader.vocab_size\n",
    "vocab = data_loader.vocab\n",
    "chars = data_loader.chars\n",
    "print ( \"type of 'data_loader' is %s, length is %d\" % (type(data_loader.vocab), len(data_loader.vocab)) )\n",
    "print ( \"\\n\" )\n",
    "print (\"data_loader.vocab looks like \\n%s \" % (data_loader.vocab))\n",
    "print ( \"\\n\" )\n",
    "print ( \"type of 'data_loader.chars' is %s, length is %d\" % (type(data_loader.chars), len(data_loader.chars)) )\n",
    "print ( \"\\n\" )\n",
    "print (\"data_loader.chars looks like \\n%s \" % (data_loader.chars,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Ready\n"
     ]
    }
   ],
   "source": [
    "# Define Network \n",
    "rnn_size   = 512\n",
    "num_layers = 3\n",
    "grad_clip  = 5.\n",
    "\n",
    "_batch_size = 1\n",
    "_seq_length = 1\n",
    "\n",
    "vocab_size = data_loader.vocab_size\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    # Select RNN Cell\n",
    "    unitcell = tf.nn.rnn_cell.BasicLSTMCell(rnn_size)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([unitcell] * num_layers)\n",
    "    # Set paths to the graph \n",
    "    input_data = tf.placeholder(tf.int32, [_batch_size, _seq_length])\n",
    "    targets    = tf.placeholder(tf.int32, [_batch_size, _seq_length])\n",
    "    initial_state = cell.zero_state(_batch_size, tf.float32)\n",
    "\n",
    "    # Set Network\n",
    "    with tf.variable_scope('rnnlm'):\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size])\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])\n",
    "            inputs = tf.split(1, _seq_length, tf.nn.embedding_lookup(embedding, input_data))\n",
    "            inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "            \n",
    "    # Loop function for seq2seq\n",
    "    def loop(prev, _):\n",
    "        prev = tf.nn.xw_plus_b(prev, softmax_w, softmax_b)\n",
    "        prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))\n",
    "        return tf.nn.embedding_lookup(embedding, prev_symbol)\n",
    "    # Output of RNN \n",
    "    outputs, last_state = tf.nn.seq2seq.rnn_decoder(inputs, initial_state, cell, loop_function=None, scope='rnnlm')\n",
    "    output = tf.reshape(tf.concat(1, outputs), [-1, rnn_size])\n",
    "    logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "    # Next word probability \n",
    "    probs = tf.nn.softmax(logits)\n",
    "    # Define LOSS\n",
    "    loss = tf.nn.seq2seq.sequence_loss_by_example([logits], # Input\n",
    "        [tf.reshape(targets, [-1])], # Target\n",
    "        [tf.ones([_batch_size * _seq_length])], # Weight \n",
    "        vocab_size)\n",
    "    # Define Optimizer\n",
    "    cost = tf.reduce_sum(loss) / _batch_size / _seq_length\n",
    "    final_state = last_state\n",
    "    lr = tf.Variable(0.0, trainable=False)\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    _optm = tf.train.AdamOptimizer(lr)\n",
    "    optm = _optm.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "print (\"Network Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample ! \n",
    "# Sampling function \n",
    "def sample( sess, chars, vocab, __probs, num=200, prime=u'ㅇㅗᴥㄴㅡㄹᴥ '):\n",
    "    # state = cell.zero_state(1, tf.float32).eval()\n",
    "    state = sess.run(cell.zero_state(1, tf.float32))\n",
    "    _probs = __probs\n",
    "    \n",
    "    prime = list(prime)\n",
    "\n",
    "    for char in prime[:-1]:\n",
    "        x = np.zeros((1, 1))\n",
    "        x[0, 0] = vocab[char]\n",
    "        feed = {input_data: x, initial_state:state}\n",
    "        [state] = sess.run([final_state], feed)\n",
    "\n",
    "    def weighted_pick(weights):\n",
    "        weights = weights / np.sum(weights) \n",
    "        \n",
    "        # print (\"weight's shape is %s\" % (weights.shape,))\n",
    "        # print (\"weight's sum is %f\" % (np.sum(weights)))\n",
    "        # weights = np.max(weights, 1e-10)\n",
    "        \n",
    "        weights = weights / np.sum(weights) \n",
    "        t = np.cumsum(weights)\n",
    "        s = np.sum(weights)\n",
    "        return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "    ret = prime\n",
    "    char = prime[-1]\n",
    "    for n in range(num):\n",
    "        x = np.zeros((1, 1))\n",
    "        x[0, 0] = vocab[char]\n",
    "        feed = {input_data: x, initial_state:state}\n",
    "        [_probsval, state] = sess.run([_probs, final_state], feed)\n",
    "        p = _probsval[0]\n",
    "        sample = int(np.random.choice(len(p), p=p))\n",
    "        # sample = weighted_pick(p)\n",
    "        # sample = np.argmax(p)\n",
    "        pred = chars[sample]\n",
    "        ret += pred\n",
    "        char = pred\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prime Text : 오늘은 => ㅇㅗᴥㄴㅡㄹᴥㅇㅡㄴᴥ\n",
      "/tmp/tf_logs/char_rnn_hangul/model.ckpt-2000\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "u'\\u3147'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-9f293471cdf4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mckpt\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mckpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0msampled_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[1;31m#print (\"\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34mu\"SAMPLED TEXT = %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msampled_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-22e2daeaaa6b>\u001b[0m in \u001b[0;36msample\u001b[1;34m(sess, chars, vocab, __probs, num, prime)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprime\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mfeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfinal_state\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: u'\\u3147'"
     ]
    }
   ],
   "source": [
    "save_dir = '/tmp/tf_logs/char_rnn_hangul'\n",
    "prime = decompose_text(u\"오늘은\")\n",
    "\n",
    "print (\"Prime Text : %s => %s\" % (automata(prime), \"\".join(prime)))\n",
    "n = 1000\n",
    "\"\"\"\n",
    "with open(os.path.join(save_dir, 'config.pkl'), 'rb') as f:\n",
    "    saved_args = cPickle.load(f)\n",
    "with open(os.path.join(save_dir, 'chars_vocab.pkl'), 'rb') as f:\n",
    "    chars, vocab = cPickle.load(f)\n",
    "\"\"\"\n",
    "    \n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "saver = tf.train.Saver(tf.all_variables())\n",
    "ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "\n",
    "print (ckpt.model_checkpoint_path)\n",
    "#print chars\n",
    "#print vocab\n",
    "\n",
    "\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    sampled_text = sample(sess, chars, vocab, probs, n, prime)\n",
    "    #print (\"\")\n",
    "    print (u\"SAMPLED TEXT = %s\" % sampled_text)\n",
    "\n",
    "    print (\"\")\n",
    "    print (\"-- RESULT --\")\n",
    "    print (automata(\"\".join(sampled_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
