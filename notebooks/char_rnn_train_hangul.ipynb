{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages Imported\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Import Packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "from TextLoader import *\n",
    "from Hangulpy import *\n",
    "print (\"Packages Imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading preprocessed files\n",
      "type of 'data_loader' is <type 'dict'>, length is 78\n",
      "\n",
      "\n",
      "data_loader.vocab looks like \n",
      "{'\\x81': 15, '\\x80': 53, '\\x83': 49, '\\x82': 20, '\\x85': 1, '\\x84': 5, '\\x87': 7, '\\x86': 25, '\\x89': 43, '\\x88': 18, '\\x8b': 45, '\\n': 22, '\\x8d': 34, '\\x8c': 33, '\\x8f': 8, '\\x8e': 17, '\\x91': 37, '\\x90': 24, '\\x93': 14, '\\x92': 74, '\\x95': 21, '\\x94': 23, '\\x97': 13, '\\x96': 42, '\\x99': 55, '\\x98': 29, '\\x9b': 41, '\\x9a': 36, '\\x9d': 39, '\\x9c': 19, '\\x9f': 38, '\\x9e': 63, '\\xa1': 12, ' ': 6, '\\xa3': 10, '\\xa2': 28, '\\xa5': 3, \"'\": 51, '>': 77, ')': 48, '(': 47, '-': 67, ',': 30, '.': 27, '\\xb1': 11, '0': 75, '\\xb3': 64, '\\xb2': 32, '\\xb5': 52, '\\xb4': 2, '\\xb7': 16, '\\xb6': 46, '\\xb9': 9, '\\xb8': 35, '\\xbb': 57, '\\xba': 50, '\\xbc': 56, '?': 44, '4': 69, '!': 54, '\\x8a': 26, '7': 71, '\\xa0': 40, '6': 61, '3': 68, '9': 66, '\"': 31, '8': 70, '2': 62, '_': 73, '\\xe1': 4, '\\xe3': 0, ':': 59, '\\x1a': 76, '1': 60, '\\xbf': 58, '\\xbe': 72, '5': 65} \n",
      "\n",
      "\n",
      "type of 'data_loader.chars' is <type 'tuple'>, length is 78\n",
      "\n",
      "\n",
      "data_loader.chars looks like \n",
      "('\\xe3', '\\x85', '\\xb4', '\\xa5', '\\xe1', '\\x84', ' ', '\\x87', '\\x8f', '\\xb9', '\\xa3', '\\xb1', '\\xa1', '\\x97', '\\x93', '\\x81', '\\xb7', '\\x8e', '\\x88', '\\x9c', '\\x82', '\\x95', '\\n', '\\x94', '\\x90', '\\x86', '\\x8a', '.', '\\xa2', '\\x98', ',', '\"', '\\xb2', '\\x8c', '\\x8d', '\\xb8', '\\x9a', '\\x91', '\\x9f', '\\x9d', '\\xa0', '\\x9b', '\\x96', '\\x89', '?', '\\x8b', '\\xb6', '(', ')', '\\x83', '\\xba', \"'\", '\\xb5', '\\x80', '!', '\\x99', '\\xbc', '\\xbb', '\\xbf', ':', '1', '6', '2', '\\x9e', '\\xb3', '5', '9', '-', '3', '4', '8', '7', '\\xbe', '_', '\\x92', '0', '\\x1a', '>') \n"
     ]
    }
   ],
   "source": [
    "data_dir    = \"data/nine_dreams\"\n",
    "batch_size  = 50\n",
    "seq_length  = 50\n",
    "data_loader = TextLoader(data_dir, batch_size, seq_length)\n",
    "# This makes \"vocab.pkl\" and \"data.npy\" in \"data/nine_dreams\"   \n",
    "#  from \"data/nine_dreams/input.txt\" \n",
    "vocab_size = data_loader.vocab_size\n",
    "vocab = data_loader.vocab\n",
    "chars = data_loader.chars\n",
    "print ( \"type of 'data_loader' is %s, length is %d\" % (type(data_loader.vocab), len(data_loader.vocab)) )\n",
    "print ( \"\\n\" )\n",
    "print (\"data_loader.vocab looks like \\n%s \" % (data_loader.vocab))\n",
    "print ( \"\\n\" )\n",
    "print ( \"type of 'data_loader.chars' is %s, length is %d\" % (type(data_loader.chars), len(data_loader.chars)) )\n",
    "print ( \"\\n\" )\n",
    "print (\"data_loader.chars looks like \\n%s \" % (data_loader.chars,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Ready\n"
     ]
    }
   ],
   "source": [
    "rnn_size   = 512\n",
    "num_layers = 3\n",
    "grad_clip  = 5.\n",
    "\n",
    "vocab_size = data_loader.vocab_size\n",
    "\n",
    "# Select RNN Cell\n",
    "unitcell = tf.nn.rnn_cell.BasicLSTMCell(rnn_size)\n",
    "cell = tf.nn.rnn_cell.MultiRNNCell([unitcell] * num_layers)\n",
    "\n",
    "# Set paths to the graph\n",
    "input_data = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "targets    = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "# Set Network\n",
    "with tf.variable_scope('rnnlm'):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])\n",
    "        inputs = tf.split(1, seq_length, tf.nn.embedding_lookup(embedding\n",
    "                    , input_data))\n",
    "        inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "# Loop function for seq2seq\n",
    "def loop(prev, _):\n",
    "    prev = tf.nn.xw_plus_b(prev, softmax_w, softmax_b)\n",
    "    prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))\n",
    "    return tf.nn.embedding_lookup(embedding, prev_symbol)\n",
    "# Output of RNN\n",
    "outputs, last_state = tf.nn.seq2seq.rnn_decoder(inputs, initial_state\n",
    "                        , cell, loop_function=None, scope='rnnlm')\n",
    "output = tf.reshape(tf.concat(1, outputs), [-1, rnn_size])\n",
    "logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "# Next word probability\n",
    "probs = tf.nn.softmax(logits)\n",
    "# Define LOSS\n",
    "loss = tf.nn.seq2seq.sequence_loss_by_example([logits], # Input\n",
    "    [tf.reshape(targets, [-1])], # Target\n",
    "    [tf.ones([batch_size * seq_length])], # Weight\n",
    "    vocab_size)\n",
    "# Define Optimizer\n",
    "cost = tf.reduce_sum(loss) / batch_size / seq_length\n",
    "final_state = last_state\n",
    "lr = tf.Variable(0.0, trainable=False)\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "_optm = tf.train.AdamOptimizer(lr)\n",
    "optm = _optm.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "print (\"Network Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/27500 (epoch: 0), loss: 5.567, time/batch: 2.846\n",
      "model saved to /tmp/tf_logs/char_rnn_hangul/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Train the model!\n",
    "num_epochs    = 50\n",
    "save_every    = 1000\n",
    "learning_rate = 0.0002\n",
    "decay_rate    = 0.97\n",
    "\n",
    "save_dir = '/tmp/tf_logs/char_rnn_hangul'\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "summary_writer = tf.train.SummaryWriter(save_dir\n",
    "                    , graph=sess.graph)\n",
    "saver = tf.train.Saver(tf.all_variables())\n",
    "for e in range(num_epochs): # for all epochs\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    sess.run(tf.assign(lr, learning_rate * (decay_rate ** e)))\n",
    "\n",
    "    data_loader.reset_batch_pointer()\n",
    "    state = sess.run(initial_state)\n",
    "    for b in range(data_loader.num_batches):\n",
    "        start = time.time()\n",
    "        x, y = data_loader.next_batch()\n",
    "        feed = {input_data: x, targets: y, initial_state: state}\n",
    "        # Train!\n",
    "        train_loss, state, _ = sess.run([cost, final_state, optm], feed)\n",
    "        end = time.time()\n",
    "\n",
    "        if b % 100 == 0:\n",
    "            print (\"%d/%d (epoch: %d), loss: %.3f, time/batch: %.3f\"  \n",
    "                   % (e * data_loader.num_batches + b\n",
    "                    , num_epochs * data_loader.num_batches\n",
    "                    , e, train_loss, end - start))\n",
    "\n",
    "        if (e * data_loader.num_batches + b) % save_every == 0:\n",
    "            checkpoint_path = os.path.join(save_dir, 'model.ckpt')\n",
    "            saver.save(sess, checkpoint_path\n",
    "                       , global_step = e * data_loader.num_batches + b)\n",
    "            print(\"model saved to {}\".format(checkpoint_path))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
